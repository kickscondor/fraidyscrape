#!/usr/bin/env node
const ent = require('ent/decode')
const fetch = require('node-fetch')
const fraidyscrape = require('..')
const fs = require('fs')
const { DOMParser, XMLSerializer } = require('@xmldom/xmldom')
const util = require('util')
const xpath = require('xpath');

function innerHtml(node) {
  let v = node.value || (node.nodeValue ? ent(node.nodeValue) : null)
  if (v) return v

  if (node.hasChildNodes())
  {
    v = ''
    for (let c = 0; c < node.childNodes.length; c++) {
      let n = node.childNodes[c]
      v += n.value || (n.nodeValue ? ent(n.nodeValue) : n.toString())
    }
  }
  return v
}

function xpathSelect(doc, node, path, asText, namespaces) {
  let result = xpath.parse(path).select({node, allowAnyNamespaceForNoPrefix: true,
    caseInsensitive: true, namespaces})
  if (asText)
    return result.map(innerHtml)
  return result
}

function parseDom(str, mime) {
  return new DOMParser().parseFromString(str, mime)
}

function parseDomFragment(str, mime) {
  let frag = false
  str = str.trim()
  if (str[0] !== '<') {
    frag = true
    str = "<div>" + str + "</div>"
  }
  let doc = new DOMParser().parseFromString(str, mime)
  if (frag) {
    let docf = doc.createDocumentFragment()
    let cn = doc.childNodes[0].firstChild
    while (cn) {
      let sib = cn.nextSibling
      docf.appendChild(cn)
      cn = sib
    }
    return docf
  }
  return doc
}

(async function () {
  var url = process.argv[3], req, obj
  var rules = JSON.parse(fs.readFileSync(process.argv[2]))
  var scraper = new fraidyscrape(rules, parseDom, xpathSelect)
  if (url.includes("://")) {
    var tasks = scraper.detect(url)
    console.log(tasks)

    while (req = scraper.nextRequest(tasks)) {
      console.log(req)
      let res = await fetch(req.url, req.options)
      if (!res.ok) {
        console.log(`${req.url} is giving a ${res.status} error.`)
      } else {
        // console.log(await res.text())
        obj = await scraper.scrape(tasks, req, res)
        if (obj.out && res.headers) {
          obj.out.etag = res.headers.get('etag')
            || res.headers.get('last-modified')
            || res.headers.get('date')
        }
        console.log(util.inspect(obj.out, false, null, true))
      }
    }
  } else if (url in rules) {
    let html = fs.readFileSync(process.argv[4], 'utf-8')
    let vars = {mime: 'text/html'}
    vars.doc = scraper.parseHtml(html, vars.mime)
    console.log(await scraper.scanSite(vars, rules[url], vars.doc))
  } else {
    var exp = JSON.parse(fs.readFileSync(url))
    for (let f of Object.values(exp.follows)) {
      if (!f.url) continue
      let tasks = scraper.detect(f.url)
      while (req = scraper.nextRequest(tasks)) {
        let res = await fetch(req.url, req.options)
        // console.log(await res.text())
        obj = await scraper.scrape(tasks, req, res)
      }
      console.log([f.url, obj.out && obj.out.posts ? obj.out.posts.length : 0])
    }
  }
})();
