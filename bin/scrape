#!/usr/bin/env node
const ent = require('ent/decode')
const fetch = require('node-fetch')
const fraidyscrape = require('..')
const fs = require('fs')
const dom = require('xmldom').DOMParser
const xpath = require('xpath');

function innerHtml(node) {
  let v = node.value || (node.nodeValue ? ent(node.nodeValue) : null)
  if (v) return v

  if (node.hasChildNodes())
  {
    v = ''
    for (let c = 0; c < node.childNodes.length; c++) {
      let n = node.childNodes[c]
      v += n.value || (n.nodeValue ? ent(n.nodeValue) : n.toString())
    }
  }
  return v
}

function xpathSelect(doc, node, path, asText, namespaces) {
  let result = xpath.parse(path).select({node, allowAnyNamespaceForNoPrefix: true,
    caseInsensitive: true, namespaces})
  if (asText)
    return result.map(innerHtml)
  return result
}

function parseDom(str, mime) {
  return new dom().parseFromString(str, mime)
}

(async function () {
  var url = process.argv[3], req
  var scraper = new fraidyscrape(JSON.parse(fs.readFileSync(process.argv[2])),
    parseDom, xpathSelect)
  var tasks = scraper.detect(url)
  console.log(tasks)

  while (req = scraper.nextRequest(tasks)) {
		console.log(req)
		let res = await fetch(req.url, req.options)
		// console.log(await res.text())
		let obj = await scraper.scrape(tasks, req, res)
		console.log(obj.out)
  }
})();
