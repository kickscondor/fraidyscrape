#!/usr/bin/env node
const ent = require('ent/decode')
const fetch = require('node-fetch')
const fraidyscrape = require('..')
const fs = require('fs')
const dom = require('xmldom').DOMParser
const xpath = require('xpath');

function innerHtml(node) {
  let v = node.value || (node.nodeValue ? ent(node.nodeValue) : null)
  if (v) return v

  if (node.hasChildNodes())
  {
    v = ''
    for (let c = 0; c < node.childNodes.length; c++) {
      let n = node.childNodes[c]
      v += n.value || (n.nodeValue ? ent(n.nodeValue) : n.toString())
    }
  }
  return v
}

function xpathSelect(doc, node, path, asText, namespaces) {
  let result = xpath.parse(path).select({node, allowAnyNamespaceForNoPrefix: true,
    caseInsensitive: true, namespaces})
  if (asText)
    return result.map(innerHtml)
  return result
}

function parseDom(str, mime) {
  return new dom().parseFromString(str, mime)
}

(async function () {
  var url = process.argv[3], req, obj
  var scraper = new fraidyscrape(JSON.parse(fs.readFileSync(process.argv[2])),
    parseDom, xpathSelect)
  if (url.includes("://")) {
    var tasks = scraper.detect(url)
    console.log(tasks)

    while (req = scraper.nextRequest(tasks)) {
      console.log(req)
      let res = await fetch(req.url, req.options)
      // console.log(await res.text())
      obj = await scraper.scrape(tasks, req, res)
      console.log(obj.out)
    }
  } else {
    var exp = JSON.parse(fs.readFileSync(url))
    for (let f of Object.values(exp.follows)) {
      if (!f.url) continue
      let tasks = scraper.detect(f.url)
      while (req = scraper.nextRequest(tasks)) {
        let res = await fetch(req.url, req.options)
        // console.log(await res.text())
        obj = await scraper.scrape(tasks, req, res)
      }
      console.log([f.url, obj.out && obj.out.posts ? obj.out.posts.length : 0])
    }
  }
})();
